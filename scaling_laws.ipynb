{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation\n",
    "\n",
    "This will generate 4 files in the `dataset` folder, each containing 50000 examples of Roman numeral arithmetic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated addition dataset: dataset/addition.txt\n",
      "Generated subtraction dataset: dataset/subtraction.txt\n",
      "Generated multiplication dataset: dataset/multiplication.txt\n",
      "Generated division dataset: dataset/division.txt\n",
      "All datasets generated successfully.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from roman import toRoman, fromRoman\n",
    "\n",
    "def generate_roman_arithmetic_dataset(operation, num_examples=50000):\n",
    "    dataset = []\n",
    "    while len(dataset) < num_examples:\n",
    "        if operation == \"/\":\n",
    "            b = random.randint(1, 100)  # Divisor can be up to 100\n",
    "            result = random.randint(1, 39)  # Limit result to ensure a * b <= 3999\n",
    "            a = result * b  # This ensures clean division\n",
    "        else:\n",
    "            a = random.randint(1, 3999)\n",
    "            b = random.randint(1, 3999)\n",
    "\n",
    "            if operation == \"+\":\n",
    "                result = a + b\n",
    "            elif operation == \"-\":\n",
    "                result = max(a, b) - min(a, b)\n",
    "                a, b = max(a, b), min(a, b)\n",
    "            elif operation == \"*\":\n",
    "                result = a * b\n",
    "\n",
    "        if result > 3999:\n",
    "            continue  # Retry if result is too large\n",
    "\n",
    "        roman_a = toRoman(a)\n",
    "        roman_b = toRoman(b)\n",
    "        roman_result = toRoman(result)\n",
    "\n",
    "        example = f\"{roman_a} {operation} {roman_b} = {roman_result}\"\n",
    "        dataset.append(example)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Create dataset directory if it doesn't exist\n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "\n",
    "# Generate datasets for each operation\n",
    "operations = [\"+\", \"-\", \"*\", \"/\"]\n",
    "operation_names = [\"addition\", \"subtraction\", \"multiplication\", \"division\"]\n",
    "\n",
    "for mode in [\"train\", \"test\"]:\n",
    "    for op, name in zip(operations, operation_names):\n",
    "        dataset = generate_roman_arithmetic_dataset(op)\n",
    "        filename = f\"dataset/{mode}_{name}.txt\"\n",
    "\n",
    "        with open(filename, \"w\") as f:\n",
    "            f.write(\"\\n\".join(dataset))\n",
    "\n",
    "        print(f\"Generated {name} dataset: {filename}\")\n",
    "\n",
    "print(\"All datasets generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning on Roman Numeral Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    TextDataset,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer):\n",
    "    return TextDataset(tokenizer=tokenizer, file_path=file_path, block_size=128)\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add padding token to tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = load_dataset(\"dataset/addition.txt\", tokenizer)\n",
    "eval_dataset = load_dataset(\"dataset/subtraction.txt\", tokenizer)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-roman-math\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_steps=400,\n",
    "    save_steps=800,\n",
    "    warmup_steps=500,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./gpt2-roman-math\")\n",
    "tokenizer.save_pretrained(\"./gpt2-roman-math\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
